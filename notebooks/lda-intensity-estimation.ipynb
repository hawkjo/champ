{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kd Analysis\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = ''\n",
    "project_name = ''\n",
    "target_name = ''\n",
    "neg_control_target_name = ''\n",
    "all_channels = ['']\n",
    "data_channel = ''\n",
    "target_sequence_file = \"/shared/targets.yml\"\n",
    "nonneg_lda_weights_fpath = '/shared/yeast_beast_LDA_weights.txt'  # for microscope 3\n",
    "# nonneg_lda_weights_fpath = '/shared/bLDA_coef_nonneg.txt'  # for microscope 2 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from IPython.display import HTML, Image\n",
    "from champ import misc, intensity, initialize, seqtools, adapters_cython\n",
    "import yaml\n",
    "\n",
    "read_name_dir = os.path.join('/shared', project_name, 'read_names')\n",
    "read_names_by_seq_fpath = os.path.join(read_name_dir, 'read_names_by_seq.txt')\n",
    "out_fname = 'LDA_intensity_scores.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Specific Section\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_sequence_file) as f:\n",
    "    targets = yaml.load(f)\n",
    "\n",
    "target = targets[target_name]\n",
    "neg_control_target = targets[neg_control_target_name]\n",
    "datadir = os.path.join('results', date)\n",
    "figdir = os.path.join('figs', date)\n",
    "custom_fig_dir = os.path.join(figdir, 'custom')\n",
    "custom_results_dir = os.path.join(datadir, 'custom')\n",
    "for dpath in [custom_fig_dir, custom_results_dir]:\n",
    "    if not os.path.isdir(dpath):\n",
    "        os.makedirs(dpath)\n",
    "out_fpath = os.path.join(custom_results_dir, out_fname)\n",
    "\n",
    "print 'Image Collection Date:', date\n",
    "print 'Sequencing Project Name:', project_name\n",
    "print 'Target \"{}\":'.format(target_name), target\n",
    "print 'Neg control target \"{}\":'.format(neg_control_target_name), neg_control_target\n",
    "print 'Channels:', all_channels\n",
    "print 'Protein channel:', data_channel\n",
    "print 'Output file:', out_fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of Interest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_seqs = set()\n",
    "    \n",
    "stretch = set()\n",
    "for i in range(1, len(target)+1):\n",
    "    stretch.update(seqtools.get_stretch_of_complement_seqs(target, i))\n",
    "insertions = set()\n",
    "for i in range(1, 3):\n",
    "    insertions.update(seqtools.get_contiguous_insertion_seqs(target, i))\n",
    "for i in range(1, 3):\n",
    "    insertions.update(seqtools.get_insertion_seqs(target, i))   \n",
    "deletions = set()\n",
    "for i in range(1, 3):\n",
    "    deletions.update(seqtools.get_deletion_seqs(target, i))\n",
    "mismatches = set()\n",
    "for i in range(1, 3):\n",
    "    mismatches.update(seqtools.get_mismatch_seqs(target, i))\n",
    "six_n_pam = seqtools.get_randomized_pam_seqs(target, 4, 6)\n",
    "other_targets = set()\n",
    "for s in targets.values():\n",
    "    other_targets.add(s)\n",
    "\n",
    "interesting_seqs.update(other_targets)\n",
    "interesting_seqs.update(stretch)\n",
    "interesting_seqs.update(insertions)\n",
    "interesting_seqs.update(deletions)\n",
    "interesting_seqs.update(mismatches)\n",
    "interesting_seqs.update(six_n_pam)\n",
    "\n",
    "print(\"Interesting sequences: %d\" % len(interesting_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Interesting Sequences Files\n",
    "\n",
    "For some reason, the `read_names_by_seq.txt` file often contains sequences with extra bases on either end of the sequence we actually care about. Which is to say, the reads are not being parsed properly. This wasn't happening before and I don't know what's changed. Regardless, here we go through it, and check every single sequence in that file to see if it contains a sequence of interest as a substring. This way, we generate a custom file that contains an exact mapping between read names and interesting sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from champ.seqtools import build_interesting_sequences\n",
    "interesting_read_names = build_interesting_sequences(read_names_by_seq_fpath, interesting_seqs)\n",
    "with open('interesting_reads_by_seq.txt', 'w') as f:\n",
    "    for sequence, read_names in interesting_read_names.items():\n",
    "        f.write(\"%s\\t%s\\n\" % (sequence, \"\\t\".join(read_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many insertions or deletions to allow\n",
    "min_len = len(target) - 3\n",
    "max_len = len(target) + 3\n",
    "max_ham = 7\n",
    "\n",
    "def is_interesting_seq(seq):\n",
    "    if seq in interesting_seqs:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_read_name_fpath = os.path.join(read_name_dir, 'all_read_names.txt')\n",
    "target_read_name_fpath = os.path.join(read_name_dir, 'target_{}_read_names.txt'.format(target_name.lower()))\n",
    "perfect_target_read_name_fpath = os.path.join(read_name_dir, 'perfect_target_{}_read_names.txt'.format(target_name.lower()))\n",
    "neg_control_target_read_name_fpath = os.path.join(read_name_dir, 'perfect_target_{}_read_names.txt'.format(neg_control_target_name.lower()))\n",
    "phiX_read_name_fpath = os.path.join(read_name_dir, 'phix_read_names.txt')\n",
    "\n",
    "all_read_names = set(line.strip() for line in open(all_read_name_fpath))\n",
    "print(\"All read names: %d\" % len(all_read_names))\n",
    "target_read_names = set(line.strip() for line in open(target_read_name_fpath))\n",
    "print(\"Target read names: %d\" % len(target_read_names))\n",
    "perfect_target_read_names = set(line.strip() for line in open(perfect_target_read_name_fpath))\n",
    "print(\"Perfect target read names: %d\" % len(perfect_target_read_names))\n",
    "neg_control_target_read_names = set(line.strip() for line in open(neg_control_target_read_name_fpath))\n",
    "print(\"Negative control read names: %d\" % len(neg_control_target_read_names))\n",
    "phiX_read_names = set(line.strip() for line in open(phiX_read_name_fpath))\n",
    "print(\"Phix read names: %d\" % len(phiX_read_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_fpaths = glob.glob('*.h5')\n",
    "i = 0\n",
    "while i < len(h5_fpaths):\n",
    "    if 'PhiX' in h5_fpaths[i] or 'chip' in h5_fpaths[i]:\n",
    "        h5_fpaths.pop(i)\n",
    "    else:\n",
    "        i += 1\n",
    "h5_fpaths.sort(key=misc.parse_concentration)\n",
    "for fpath in h5_fpaths:\n",
    "    print misc.parse_concentration(fpath), fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_name = date\n",
    "results_dirs = [\n",
    "    os.path.join('results', \n",
    "                 os.path.splitext(os.path.basename(h5_fpath))[0])\n",
    "    for h5_fpath in h5_fpaths\n",
    "]\n",
    "for d in results_dirs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Loading data...'\n",
    "int_scores = intensity.IntensityScores(h5_fpaths)\n",
    "int_scores.get_LDA_scores(results_dirs, nonneg_lda_weights_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print 'Normalizing data...'\n",
    "int_scores.normalize_scores()\n",
    "print 'Done normalizing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.plot_aligned_images('br', 'o*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.plot_normalization_constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.print_reads_per_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_num_ims_cutoff = len(h5_fpaths) - 3\n",
    "int_scores.build_good_read_names(good_num_ims_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_read_names = int_scores.good_read_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_perfect_read_names = perfect_target_read_names & good_read_names\n",
    "print 'Good Reads:', len(good_read_names)\n",
    "print 'Good Perfect Reads:', len(good_perfect_read_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.build_score_given_read_name_given_channel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Collating by Sequence\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only read names with cascade scores\n",
    "print(\"Starting\")\n",
    "aligned_read_names = []\n",
    "for h5_fpath in h5_fpaths:\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    for d in int_scores.scores[h5_fpath][data_channel].values():\n",
    "        for read_name in d.keys():\n",
    "            aligned_read_names.append(read_name)\n",
    "aligned_read_names = set(aligned_read_names)\n",
    "print '\\nAligned reads in protein channel:', len(aligned_read_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    max_ham\n",
    "except:\n",
    "    max_ham = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Collating Reads by Sequence'\n",
    "interesting_reads = seqtools.build_read_names_given_seq(target,\n",
    "                                                        'interesting_reads_by_seq.txt',\n",
    "                                                        aligned_read_names,\n",
    "                                                        is_interesting_seq,\n",
    "                                                        max_ham)\n",
    "\n",
    "print(len(interesting_reads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_reads[neg_control_target].update(neg_control_target_read_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter reads with outlier intensities for their sequence identity, and filter sequences with fewer than 5 reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_reads = 5\n",
    "tukey_contant = 1.5  # Read acceptabale if in range [q1 - tukey_contant * iqr, q3 + tukey_contant * iqr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Filtering reads by intensity and seqs by final read count'\n",
    "interesting_seqs = set()\n",
    "for i, (seq, read_names) in enumerate(interesting_reads.items()):\n",
    "    if i % 10000 == 0:\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "    if len(read_names) < min_reads:\n",
    "        continue\n",
    "    for h5_fpath in h5_fpaths:\n",
    "        if data_channel not in int_scores.score_given_read_name_in_channel[h5_fpath]:\n",
    "            continue\n",
    "        score_given_read_name = int_scores.score_given_read_name_in_channel[h5_fpath][data_channel]\n",
    "        intensities = [\n",
    "            score_given_read_name[read_name]\n",
    "            for read_name in read_names\n",
    "            if read_name in score_given_read_name\n",
    "        ]\n",
    "        if len(intensities) < min_reads:\n",
    "            continue\n",
    "        q1 = np.percentile(intensities, 25)\n",
    "        q3 = np.percentile(intensities, 75)\n",
    "        iqr = q3 - q1\n",
    "        min_range, max_range = (q1 - tukey_contant * iqr, q3 + tukey_contant * iqr)\n",
    "        new_read_names = set()\n",
    "        for read_name in read_names:\n",
    "            try:\n",
    "                if min_range <= score_given_read_name[read_name] <= max_range:\n",
    "                    new_read_names.add(read_name)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    interesting_reads[seq] = new_read_names\n",
    "    if len(new_read_names) >= min_reads:\n",
    "        interesting_seqs.add(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "seqtools.plot_library_comp_by_hamming_distance(ax,\n",
    "                                               target,\n",
    "                                               max_ham,\n",
    "                                               min_reads,\n",
    "                                               interesting_reads,\n",
    "                                               interesting_seqs)\n",
    "ax.set_title('Target {} Library'.format(target_name), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Negative Control Seqs:', len(interesting_reads[neg_control_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentrations = map(misc.parse_concentration, h5_fpaths)\n",
    "print 'Concentrations:'\n",
    "concentrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_name = 'concentration_pM'\n",
    "trait_list = concentrations\n",
    "attrs_dict = {\n",
    "    'target': target, \n",
    "    'target_name': target_name,\n",
    "    'neg_control_target': neg_control_target,\n",
    "    'neg_control_target_name': neg_control_target_name,\n",
    "}\n",
    "\n",
    "int_scores.write_values_by_seq(\n",
    "    course_trait_name=trait_name,\n",
    "    course_trait_list=trait_list,\n",
    "    h5_fpaths=h5_fpaths,\n",
    "    attrs_dict=attrs_dict,\n",
    "    channel_of_interest=data_channel,\n",
    "    seqs_of_interest=interesting_seqs,\n",
    "    read_names_given_seq=interesting_reads,\n",
    "    out_fpath=out_fpath,       \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
